{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalance Problem - Glass Data (Multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('glass.csv')\n",
    "df['class'].value_counts().sort_index(ascending=False).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['class'])\n",
    "df['class'].value_counts().sort_index(ascending=False).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsX = [i for i in df.columns if i != 'class']\n",
    "X = df[colsX].values\n",
    "y = df['class'].values\n",
    "\n",
    "# Splitting data\n",
    "X_train_std, X_test_std, y_train, y_test = train_test_split(X, y,\n",
    "    test_size=0.20,\n",
    "    stratify=y,\n",
    "    random_state=1)\n",
    "\n",
    "# Standardizing\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train_std)\n",
    "X_test_std = sc.transform(X_test_std)\n",
    "\n",
    "# Constructing a pipeline object\n",
    "svc = SVC(random_state=1)\n",
    "\n",
    "# Training\n",
    "svc.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediction from test data\n",
    "y_pred = svc.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    estimator=svc, X=X_test_std, y=y_test\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "#Multilabel confusion matrix\n",
    "matrices = multilabel_confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "nc = np.unique(y).shape[0]\n",
    "fig, axs = plt.subplots(nc, 1, figsize=(5,nc*4))\n",
    "for idx, m in enumerate(matrices):\n",
    "    sns.heatmap(np.flip(m), annot=True, cmap='Blues', ax=axs[idx])\n",
    "    axs[idx].set_title(f'Class {idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred,  digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "arr = []\n",
    "for average in ['macro', 'weighted', 'micro']:\n",
    "    prfs = precision_recall_fscore_support(y_test, y_pred, average=average)\n",
    "    data = {'average': average, 'precision': prfs[0], \"recall\": prfs[1], \"f1\": prfs[2] }\n",
    "    arr.append(data)\n",
    "\n",
    "dft = pd.DataFrame.from_records(arr, index='average')\n",
    "display(dft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform gridsearch on the selected metrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "c_gamma_range = [0.01, 0.1, 1.0, 10.0]\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "set1 = {'C': param_range, 'kernel': ['linear']}\n",
    "set2 = {'C': param_range, 'gamma': param_range, 'kernel': ['rbf']}\n",
    "param_grid = [set1, set2]\n",
    "\n",
    "# Making scorer wrapper so that we can pass the desired argument.\n",
    "scorer = make_scorer(recall_score, average='micro')\n",
    "\n",
    "# Grid search.\n",
    "gs = GridSearchCV(estimator=svc,\n",
    "                  param_grid=param_grid,\n",
    "                  # Use scorer here\n",
    "                  scoring=scorer,\n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "                  \n",
    "gs = gs.fit(X_train_std, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-evaluate metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred,  digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the `class weight` setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a pipeline object\n",
    "svc_balanced = SVC(random_state=1, class_weight='balanced')\n",
    "# Training\n",
    "svc_balanced.fit(X_train_std, y_train)\n",
    "\n",
    "# Prediction from test data\n",
    "y_pred = svc_balanced.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    estimator=svc, X=X_test_std, y=y_test\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_train_os, y_train_os = oversample.fit_resample(X_train_std, y_train)\n",
    "pd.Series(y_train_os).value_counts()\n",
    "\n",
    "# Training\n",
    "svc.fit(X_train_os, y_train_os)\n",
    "# Prediction from test data\n",
    "y_pred = svc.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    estimator=svc, X=X_test_std, y=y_test\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79088bb772545dc9740b3f6fd02f1fa74686ae15b783fc1c2abf8492adb1c7fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
