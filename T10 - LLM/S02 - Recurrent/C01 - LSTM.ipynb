{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Word Prediction Using LSTM\n",
        "\n",
        "Adapted from https://github.com/Vishwaaaah/Next_word_prediction_using_LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0L1EKje2K9P8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrain the model\n",
        "#IS_TRAIN_MODE = True\n",
        "\n",
        "# Load the model from save\n",
        "IS_TRAIN_MODE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06LdyVgVLOaa"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "\n",
        "baseDir = os.getcwd()\n",
        "# curDir = os.path.join(baseDir, \"T10 - LLM\", \"S02 - Recurrent\")\n",
        "curDir = baseDir\n",
        "filePath = os.path.join(curDir, \"hamlet.txt\")\n",
        "print(filePath)\n",
        "\n",
        "if not os.path.exists(filePath):\n",
        "    import nltk\n",
        "    nltk.download('gutenberg')\n",
        "    from nltk.corpus import gutenberg\n",
        "\n",
        "    data = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
        "    with open(filePath, \"w\") as file:\n",
        "        file.write(data)\n",
        "\n",
        "\n",
        "with open(filePath) as file:\n",
        "    text = file.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the first 10 lines\n",
        "for idx, line in enumerate(text.split(\"\\n\")):\n",
        "    print(line)\n",
        "    if idx > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4qeoxurALVZg"
      },
      "outputs": [],
      "source": [
        "# Show the first 20 words as a dictionary\n",
        "for idx, (k, v) in enumerate(tokenizer.word_index.items()):\n",
        "    print(f\"{k:5s} -> {v:5d}\")\n",
        "    if idx > 20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUoHdVf3MVsI"
      },
      "outputs": [],
      "source": [
        "# Creating input-sequence\n",
        "\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[: i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAz6-Ig-NGwl"
      },
      "outputs": [],
      "source": [
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "print(max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hJnXfrNa6sY"
      },
      "outputs": [],
      "source": [
        "# Pad sequences\n",
        "input_sequences = np.array(\n",
        "    pad_sequences(input_sequences, maxlen=max_sequence_length, padding=\"pre\")\n",
        ")\n",
        "input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lqjVbNNMb9Ns"
      },
      "outputs": [],
      "source": [
        "# Create predictors and labels\n",
        "X, yt = input_sequences[:, :-1], input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihP27tBJcZz1"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "print(X[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpWGtqjNcayI"
      },
      "outputs": [],
      "source": [
        "yt[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS-6TUY6cb4i"
      },
      "outputs": [],
      "source": [
        "# Convert labels to categorical\n",
        "y = tf.keras.utils.to_categorical(yt, num_classes=total_words)\n",
        "print(y.shape)\n",
        "print(y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sdqTGZzkcrYI"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "Gvix_7XLc-wA",
        "outputId": "900d3b39-1b1d-4739-9a38-e900d181db62"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "if IS_TRAIN_MODE:\n",
        "    # Model 1\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 100))\n",
        "    model.add(Bidirectional(LSTM(150)))\n",
        "    model.add(Dense(total_words, activation=\"softmax\"))\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    #  accuracy: 0.8373 - loss: 0.6894\n",
        "\n",
        "    # Model 2\n",
        "    # model = Sequential()\n",
        "    # model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
        "    # model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "    # model.add(Dropout(0.3))\n",
        "    # model.add(Bidirectional(LSTM(units=64)))\n",
        "    # model.add(Dropout(0.3))\n",
        "    # model.add(BatchNormalization())\n",
        "    # model.add(Dense(64, activation='relu'))\n",
        "    # model.add(Dense(total_words, activation='softmax'))\n",
        "    # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.build(input_shape=(None, max_sequence_length - 1))\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNFUVitdgdC",
        "outputId": "9b37a440-7475-44f5-bfc9-ac6fc37713df"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "if IS_TRAIN_MODE:\n",
        "    history = model.fit(x_train, y_train, epochs=40, verbose=1)\n",
        "    # history=model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test),verbose=1)\n",
        "\n",
        "    # Save the model\n",
        "    dateTime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    model.save(os.path.join(curDir, f\"model-{dateTime}.keras\"))\n",
        "\n",
        "    # Save tokenizer\n",
        "    with open(os.path.join(curDir, \"tokenizer.pkl\"), \"wb\") as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "if not IS_TRAIN_MODE:\n",
        "    model = tf.keras.models.load_model('model-20250303-050608.keras')\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MOwAXmIJdb-F"
      },
      "outputs": [],
      "source": [
        "# Function to predict next word\n",
        "\n",
        "def predict_next_word(model, tokenizer, text):\n",
        "    max_sequence_length = model.input_shape[1] + 1\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    if len(token_list) >= max_sequence_length:\n",
        "        token_list = token_list[-(max_sequence_length):]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_length - 1, padding=\"pre\"\n",
        "    )\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLxz3Ehy0PJl",
        "outputId": "296cfa0d-4417-4842-a65f-1c813ef80f7f"
      },
      "outputs": [],
      "source": [
        "# Predict next word\n",
        "\n",
        "input_text = \"With mirth the king\"\n",
        "print(f\"Input text: {input_text}\")\n",
        "#\n",
        "token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
        "print(f\"Padded token list: {token_list}\")\n",
        "#\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding=\"pre\")\n",
        "print(f\"Token list: {token_list}\")\n",
        "#\n",
        "predicted = model.predict(token_list, verbose=0)\n",
        "print(f\"Predicted: {predicted}\")\n",
        "#\n",
        "predicted_word_index = np.argmax(predicted, axis=1)\n",
        "print(f\"Predicted word index: {predicted_word_index}\")\n",
        "#\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted_word_index:\n",
        "        print(f\"Predicted next word: {word}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"I am\"\n",
        "\n",
        "textStr = input_text\n",
        "print(textStr, end=\" \")\n",
        "for i in range(1, 150):\n",
        "    next_word = predict_next_word(model, tokenizer, textStr)\n",
        "    print(next_word, end=\" \")\n",
        "    if i % 20 == 0:\n",
        "        print(\"\\n\", end=\"\")\n",
        "    textStr = textStr + \" \" + next_word"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
