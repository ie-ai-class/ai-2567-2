{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Word Prediction Using LSTM\n",
        "\n",
        "https://github.com/Vishwaaaah/Next_word_prediction_using_LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0L1EKje2K9P8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "zE9j9by8LLrX"
      },
      "outputs": [],
      "source": [
        "# nltk.download(\"gutenberg\")\n",
        "# from nltk.corpus import gutenberg\n",
        "# data = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
        "# with open(\"hamlet.txt\", \"w\") as file:\n",
        "#     file.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "06LdyVgVLOaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nr\\Coding\\ai-class\\ai-2567-2\\T10 - LLM\\S02 - Recurrent\\hamlet.txt\n"
          ]
        }
      ],
      "source": [
        "baseDir = os.getcwd()\n",
        "curDir = os.path.join(baseDir, \"T10 - LLM\", \"S02 - Recurrent\")\n",
        "filePath = os.path.join(curDir, \"hamlet.txt\")\n",
        "print(filePath)\n",
        "\n",
        "if not os.path.exists(filePath):\n",
        "    raise FileNotFoundError(\"File not found\")\n",
        "\n",
        "with open(filePath) as file:\n",
        "    text = file.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[the tragedie of hamlet by william shakespeare 1599]\n",
            "\n",
            "\n",
            "actus primus. scoena prima.\n",
            "\n",
            "enter barnardo and francisco two centinels.\n",
            "\n",
            "  barnardo. who's there?\n",
            "  fran. nay answer me: stand & vnfold\n",
            "your selfe\n",
            "\n",
            "   bar. long liue the king\n"
          ]
        }
      ],
      "source": [
        "for idx, line in enumerate(text.split(\"\\n\")):\n",
        "    print(line)\n",
        "    if idx > 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4818\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "4qeoxurALVZg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the   ->     1\n",
            "and   ->     2\n",
            "to    ->     3\n",
            "of    ->     4\n",
            "i     ->     5\n",
            "you   ->     6\n",
            "a     ->     7\n",
            "my    ->     8\n",
            "it    ->     9\n",
            "in    ->    10\n",
            "that  ->    11\n",
            "ham   ->    12\n",
            "is    ->    13\n",
            "not   ->    14\n",
            "his   ->    15\n",
            "this  ->    16\n",
            "with  ->    17\n",
            "your  ->    18\n",
            "but   ->    19\n",
            "for   ->    20\n",
            "me    ->    21\n",
            "lord  ->    22\n"
          ]
        }
      ],
      "source": [
        "for idx, (k, v) in enumerate(tokenizer.word_index.items()):\n",
        "    print(f\"{k:5s} -> {v:5d}\")\n",
        "    if idx > 20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fUoHdVf3MVsI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 687],\n",
              " [1, 687, 4],\n",
              " [1, 687, 4, 45],\n",
              " [1, 687, 4, 45, 41],\n",
              " [1, 687, 4, 45, 41, 1886]]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating input-sequence\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[: i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BAz6-Ig-NGwl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "max_sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5hJnXfrNa6sY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    1,  687],\n",
              "       [   0,    0,    0, ...,    1,  687,    4],\n",
              "       [   0,    0,    0, ...,  687,    4,   45],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    4,   45, 1047],\n",
              "       [   0,    0,    0, ...,   45, 1047,    4],\n",
              "       [   0,    0,    0, ..., 1047,    4,  193]])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences = np.array(\n",
        "    pad_sequences(input_sequences, maxlen=max_sequence_length, padding=\"pre\")\n",
        ")\n",
        "input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "lqjVbNNMb9Ns"
      },
      "outputs": [],
      "source": [
        "# Create predictors and labels\n",
        "X, yt = input_sequences[:, :-1], input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ihP27tBJcZz1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25732, 13)\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   1]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1 687]\n",
            " [  0   0   0   0   0   0   0   0   0   0   1 687   4]\n",
            " [  0   0   0   0   0   0   0   0   0   1 687   4  45]\n",
            " [  0   0   0   0   0   0   0   0   1 687   4  45  41]]\n"
          ]
        }
      ],
      "source": [
        "print(X.shape)\n",
        "print(X[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "DpWGtqjNcayI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 687,    4,   45,   41, 1886])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yt[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "oS-6TUY6cb4i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25732, 4818)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "y = tf.keras.utils.to_categorical(yt, num_classes=total_words)\n",
        "print(y.shape)\n",
        "print(y[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "sdqTGZzkcrYI"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "Gvix_7XLc-wA",
        "outputId": "900d3b39-1b1d-4739-9a38-e900d181db62"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">481,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">301,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4818</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,450,218</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m481,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │         \u001b[38;5;34m301,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4818\u001b[0m)                │       \u001b[38;5;34m1,450,218\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,233,218</span> (8.52 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,233,218\u001b[0m (8.52 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,233,218</span> (8.52 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,233,218\u001b[0m (8.52 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Model 1\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "#  accuracy: 0.8373 - loss: 0.6894\n",
        "\n",
        "# Model 2\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
        "# model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Bidirectional(LSTM(units=64)))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(total_words, activation='softmax'))\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.build(input_shape=(None, max_sequence_length - 1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNFUVitdgdC",
        "outputId": "9b37a440-7475-44f5-bfc9-ac6fc37713df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.0326 - loss: 7.0962\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=1, verbose=1)\n",
        "# history=model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test),verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "MOwAXmIJdb-F"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(model, tokenizer, text, max_sequence_length):\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    if len(token_list) >= max_sequence_length:\n",
        "        token_list = token_list[-(max_sequence_length):]\n",
        "    token_list = pad_sequences(\n",
        "        [token_list], maxlen=max_sequence_length - 1, padding=\"pre\"\n",
        "    )\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    predicted_word_index = np.argmax(predicted, axis=1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_word_index:\n",
        "            return word\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z98VdEbuiIcR",
        "outputId": "6da686ce-195f-415d-d102-bbf3241fac37"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(os.path.join(curDir, \"my_model.keras\"))\n",
        "\n",
        "# Save tokenizer\n",
        "with open(os.path.join(curDir, \"tokenizer.pkl\"), \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLxz3Ehy0PJl",
        "outputId": "296cfa0d-4417-4842-a65f-1c813ef80f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: With mirth the king\n",
            "Next word: of\n"
          ]
        }
      ],
      "source": [
        "input_text = \"With mirth the king\"\n",
        "print(f\"Input text: {input_text}\")\n",
        "max_sequence_length = model.input_shape[1] + 1\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
        "print(f\"Next word: {next_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDJDmqgRz9sT",
        "outputId": "65359ca6-e3e2-42ea-be6d-d3752baad3be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: In the\n",
            "Next word: king\n"
          ]
        }
      ],
      "source": [
        "input_text = \"In the\"\n",
        "print(f\"Input text: {input_text}\")\n",
        "max_sequence_length = model.input_shape[1] + 1\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
        "print(f\"Next word: {next_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrMR4UClAGpt",
        "outputId": "12c182e8-cd3d-4feb-a20c-60c78375c640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: Taken to\n",
            "Next word: the\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Taken to\"\n",
        "print(f\"Input text: {input_text}\")\n",
        "max_sequence_length = model.input_shape[1] + 1\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
        "print(f\"Next word: {next_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vU50XJTBf_-",
        "outputId": "517c5bbc-552f-491c-d471-95bd44db8b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input text: I am\n",
            "Next word: the\n"
          ]
        }
      ],
      "source": [
        "input_text = \"I am\"\n",
        "# print(\"original text: Laertes, and his sister\")\n",
        "print(f\"Input text: {input_text}\")\n",
        "max_sequence_length = model.input_shape[1] + 1\n",
        "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
        "print(f\"Next word: {next_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the king of the king of the king of the king of stone betime the rat betime of the betime of of betime the betime stone stone outherod's stone outherod's outherod's outherod's the rat stone traitorous traitorous of of rat traitorous traitorous traitorous traitorous traitorous traitorous traitorous traitorous traitorous traitorous "
          ]
        }
      ],
      "source": [
        "input_text = \"I am\"\n",
        "for i in range(50):\n",
        "    max_sequence_length = model.input_shape[1] + 1\n",
        "    next_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
        "    print(next_word, end=\" \")\n",
        "    input_text = input_text + \" \" + next_word"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
